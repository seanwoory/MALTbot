# 2026-03-01 Daily Research Strategy Update

## 1. 최근 성능 추이 (Recent Performance Trends, Last 1-3 Days)
- **2026-02-26 ~ 02-27**: `chgnet_pretrained_infer` 모델은 4.754 MAE에서 정체 상태였으며, 대부분의 finetuning 시도는 `ERROR`로 실패했습니다.
- **2026-02-28**: `chgnet_head_finetune_freeze` (백본 동결, Readout + AtomRef 미세 조정) 시도가 성공적으로 동작하여 오차를 **0.467452 MAE**로 대폭 감소시켰습니다. 반면 `chgnet_full_finetune`은 4.903 MAE로 성능이 저조했으며, `alignn_fold0_agile` 실행은 SKIPPED 되었습니다.
- **결론**: 완전히 동결을 푼 Full Finetuning은 불안정하고 성능 저하를 야기합니다. 현재 가장 성공적인 레시피는 '부분 동결 기반 Head Finetuning'입니다.

## 2. 기본 타깃 및 제안 (Targeting)
- **기본 타깃**: `mp_e_form` (최상위 목표: MAE ~0.02 수준 달성)
- Head finetuning이 효과적임을 확인했으나 여전히 SOTA와는 거리가 있으므로, mp_e_form에서 안정적인 성능 향상이 둔화된다면 `mp_gap` 모델 학습으로 전환하여 전이 학습 전략을 병행하는 것을 제안합니다.

## 3. 다음 24시간 최선 전략 (Top Strategy for Next 24h)
- **전략**: **CHGNet 부분 백본 해제 (Partial Backbone Unfreezing) + 낮은 학습률 적용**
- **왜 (Why)**: Head(Readout)만 학습하는 것으로는 표현력에 한계가 옴(0.467). 반면 Full Finetune은 학습 초기 불안정성으로 4.903 MAE로 붕괴함. 따라서 가장 마지막 GNN 레이어 1~2개만 unfreeze하여 점진적으로 모델 수용력을 늘리는 방법이 최적.
- **리스크 (Risk)**: Unfreeze 하는 레이어가 너무 많으면 이전 Full Finetune 처럼 weight가 붕괴할 수 있음.
- **실행 단계**: 
  1. Head 부분은 기존처럼 학습 가능하게 둠.
  2. CHGNet Backbone의 마지막 Message Passing layer만 `requires_grad=True`로 해제.
  3. Optimizer의 Learning Rate를 1e-4 또는 그 이하로 낮게 설정하여 안정성을 확보.

## 4. 오늘 AM/PM 2-run 계획 (구체화, 1개 변수 원칙)
- **AM Run**: `chgnet_head_finetune_freeze` (어제 베스트 모델 베이스) + **Learning Rate 1/10 감소 (ex. 1e-3 -> 1e-4)**.
  - 변수 1개: Head의 Learning Rate. (Head가 더 낮은 Loss로 수렴할 수 있는지 테스트)
- **PM Run**: `chgnet_partial_finetune_last_layer` (Head + 마지막 레이어 1개 Unfreeze) + **AM의 베스트 LR 고정**.
  - 변수 1개: 백본의 Unfreeze 깊이(0에서 1레이어로). (AM의 결과가 좋든 나쁘든, 네트워크 Capacity를 확장하여 SOTA를 추적)