# MALTbot Research Strategy Update (2026-02-25)

## 1. 최근 1~3일 성능 추이 요약
- `RESULTS.md` 확인 결과, `mp_e_form` 태스크에서 `chgnet_pretrained_infer` (Zero-shot) 모델이 MAE 4.754를 기록했습니다.
- 반면 `chgnet_head_finetune_freeze`와 `chgnet_full_finetune` 학습은 최근 이틀 연속(2/23~2/24) `ERROR`가 발생하며 실패하고 있습니다.
- 현재까지 유효한 최고 성능은 2월 20일의 `structure_simple+HGB` 베이스라인(MAE 0.493)이며, 딥러닝 모델 적용에 난항을 겪고 있습니다.

## 2. 기본 타깃 및 병행 제안
- **기본 타깃**: `mp_e_form` (유지)
- **병행 제안**: 현재 `mp_e_form` 학습 파이프라인(CHGNet)에 구조적 에러가 발생하고 있으므로, 에러 해결이 장기화될 경우 상대적으로 학습이 용이할 수 있는 `mp_gap` 밴드갭 예측 태스크로 임시 전환하여 파이프라인 안정성을 테스트하는 것을 제안합니다.

## 3. 다음 24h 최선 전략 (Top-1 달성 목적)
- **전략**: `chgnet_head_finetune_freeze` 에러 디버깅 및 파이프라인 정상화
- **왜 (Why)**: CHGNet 기반 모델의 Fine-tuning은 Top-1 달성을 위한 필수 단계입니다. 백본과 구조적 연결 과정의 버그를 우선 해결해야 다음 스텝(Full Fine-tuning)으로 넘어갈 수 있습니다.
- **리스크**: GPU 메모리 초과, PyTorch/DGL 의존성 충돌, 또는 CHGNet 데이터 로더의 호환성 문제일 가능성이 높아 디버깅에 시간이 소요될 수 있습니다.
- **실행 단계**:
  1. 가장 최근 실패한 `chgnet_head_finetune_freeze`의 로그 및 에러 트레이스백 분석
  2. 에러 원인 수정 (데이터 로딩, 차원 불일치, 모델 파라미터 freeze 로직 등)
  3. 스몰 데이터셋(배치 1~2개)으로 Sanity check 훈련 실행

## 4. AM/PM 2-run 계획 (바꾸는 변수 1개 원칙)
- **AM Run**: `chgnet_head_finetune_freeze` 버그 수정 후 재실행
  - **변수**: 코드 버그 수정 (동일한 모델 구조 유지, 에러 해결에 집중)
- **PM Run**: `chgnet_full_finetune` 실행
  - **변수**: 파라미터 동결 여부 (AM 런이 성공하면, PM에는 백본까지 모두 학습하는 full finetuning으로 전환하여 성능 비교)
