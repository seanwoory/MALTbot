# 2026-02-21 — mp_e_form (MAE~0.11)에서 “가장 효율적으로 성능을 올리는” TOP 3 Next Steps

## 전제(현재 파이프라인 상태)
- 현재 mp_e_form에서 기록된 실측 baseline은 **MAE ~0.11** 수준이며, 결과 파일에 모델이 `Route-B CHGNet-lite (composition MLP baseline)`로 표기되어 있음.
- `target_transform / lr_schedule / ema` 실험은 결과가 baseline과 **완전히 동일**하여, 현 단계에서는 레버가 “실제로 적용되지 않는 placeholder” 가능성이 큼.

따라서 ‘효율 극대화’의 최우선은 **(1) 가장 큰 성능 갭을 한 번에 메우는 변화**와 **(2) 1~2시간 세션에서 유효성을 즉시 검증 가능한 실험**이다.

---

## TOP 1) **Full CHGNet pretrained inference (no training) → matbench mp_e_form에 바로 적용**
> “학습 없이” 성능을 크게 끌어올릴 수 있는 최단 루트. 1~2시간 세션에서 충분히 5-fold 예측/record까지 가능(특히 GPU가 강해진 지금).

- (a) 기대 성능 이득
  - 방향: **MAE 크게 감소** (0.11 → 0.0x대로 내려갈 가능성이 높음)
  - 크기(정성): **가장 큰 점프**. 지금이 조성 MLP면 구조 기반 pretrained로 갈 때 점프 폭이 압도적.
  - 단, “Top-1(0.0170)” 달성은 별개. 하지만 baseline 갭을 메우는 가장 효율적인 1-step.

- (b) 구현 난이도
  - **중간**: 구조(`pymatgen.Structure`) → CHGNet `predict_structure` 호출 → 예측을 matbench record 포맷으로 저장.
  - 학습 루프가 없어서 오히려 단순.

- (c) 실험 설계(바꾸는 변수 1개)
  - 변수: **model swap** (CHGNet-lite MLP → `CHGNet.load()` pretrained + inference)
  - 나머지(epochs/lr 등)는 없음.

- (d) 실패 리스크 / 검증법
  - 리스크:
    - 예측 타깃이 mp_e_form의 `e_form`과 CHGNet 에너지 출력(절대에너지/퍼-아톰)의 의미가 달라서 “그대로는 맞지 않을” 수 있음.
  - 검증:
    - fold 0만 먼저 돌려서 MAE가 0.11보다 확 내려가는지 확인(부분 실행).
    - 예측 분포(평균/표준편차)가 target 분포에 비해 지나치게 좁거나(분산 붕괴) 오프셋이 큰지 확인.

---

## TOP 2) **CHGNet ‘헤드만’ 초단기 fine-tune (freeze backbone) + 아주 적은 epoch**
> 1~2시간 안에서 “학습의 효과”를 보려면 전체 fine-tune보다 **freeze + head-only**가 효율적.

- (a) 기대 성능 이득
  - 방향: **MAE 감소**
  - 크기(정성): inference-only보다 더 내려갈 가능성(타깃 정렬). 최소한 baseline 0.11 대비 큰 개선 기대.

- (b) 구현 난이도
  - **중간~높음**: 학습 루프/로더/early stop이 필요.
  - 하지만 CHGNet은 공식 fine-tuning 노트북에 freeze 예시가 있음.

- (c) 실험 설계(바꾸는 변수 1개)
  - 변수: **freeze 전략 ON** (backbone freeze + head 학습)
  - 고정: epochs(예: 5~10), lr(예: 1e-3), batch size.

- (d) 실패 리스크 / 검증법
  - 리스크:
    - 학습이 실제로 돌지 않거나(placeholder처럼), record는 되는데 성능이 안 바뀌는 상황.
    - target transform/역변환 실수.
  - 검증:
    - 1) train loss 감소 확인, 2) fold0 단독으로 빠르게 MAE 체크.

---

## TOP 3) **Seed ensemble(3 seeds) ‘가벼운 앙상블’ (학습이든 추론이든) — 분산 이득 즉시 회수**
> 오늘 결과에서 seed만 바꿔도 MAE가 ~0.0025 움직였음. 이건 앙상블로 “바로 먹을 수 있는 신호”.

- (a) 기대 성능 이득
  - 방향: **MAE 감소 + std 감소**
  - 크기(정성): 단일 대비 **0.001~0.003 수준**의 안정적 개선을 기대(현재 관측된 seed 변동폭 기준).

- (b) 구현 난이도
  - **낮음~중간**: 같은 설정으로 seed만 바꿔 3개 예측을 만들고 평균.
  - 단, 파이프라인에 ensemble 평균 record가 아직 SKIPPED로 남아있어서(미구현), 구현이 필요.

- (c) 실험 설계(바꾸는 변수 1개)
  - 변수: **ensemble_size = 3** (seed 3개)
  - 고정: 모델/하이퍼파라미터.

- (d) 실패 리스크 / 검증법
  - 리스크:
    - 파이프라인이 아직 ensemble 평균을 못 만들거나, 저장 포맷이 깨질 수 있음.
  - 검증:
    - fold0에서만 3-seed 예측 평균 → MAE가 각 seed 평균보다 내려가는지 확인.

---

## “왜 이 TOP3인가” 요약
- **(1) 모델 스왑(Full CHGNet)**은 baseline 0.11에서 구조 기반으로 넘어가는 가장 큰 점프.
- **(2) head-only fine-tune**은 1~2시간 세션에서도 학습 효과를 볼 수 있는 가장 효율적인 finetune 형태.
- **(3) seed ensemble**은 이미 관측된 seed 변동을 ‘성능’으로 바로 환전하는 방법.
