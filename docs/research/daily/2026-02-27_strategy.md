# MALTbot Research Strategy: 2026-02-27

## 1. 최근 성능 추이 (Recent Performance Summary)
- **최근 3일 (2/25~2/27)**: `matbench_mp_e_form` 태스크에서 `chgnet_pretrained_infer` (Zero-shot) 모델이 일관적으로 **MAE = 4.754132** 를 기록 중입니다.
- **문제점**: `chgnet_head_finetune_freeze` (헤드 파인튜닝) 및 `chgnet_full_finetune` (전체 파인튜닝) 파이프라인이 지속적으로 `ERROR` 상태입니다. 파인튜닝 로직에 구조적 결함(OOM, 차원 불일치 등)이 있는 것으로 보입니다.

## 2. 타깃 설정 및 병행 제안
- **기본 타깃**: `mp_e_form` 최적화 및 에러 해결.
- **병행/전환 제안**: 파인튜닝 버그가 특정 프로퍼티(e_form) 데이터셋 크기나 형태에 국한된 문제인지 파악하기 위해, 상대적으로 작고 다루기 쉬운 `mp_gap` (Band gap) 태스크로 전환하여 파이프라인의 정상 동작 여부를 테스트해 볼 것을 제안합니다.

## 3. Top-1 달성을 위한 '다음 24h' 최선 전략
- **전략**: CHGNet 헤드 파인튜닝(`chgnet_head_finetune_freeze`) 오류 디버깅 및 단일 배치 테스트.
- **왜(Why)**: Zero-shot 추론만으로는 SOTA 성능(Top-1) 달성이 불가능하며, 현재 파인튜닝 파이프라인이 완전히 붕괴되어 있으므로 모델 학습 정상화가 가장 시급합니다.
- **리스크**: 프레임워크 수준의 버그일 경우 디버깅에 시간이 소요될 수 있습니다.
- **실행단계**:
  1. `chgnet_head_finetune_freeze` 로그를 확인하여 정확한 에러 트레이스 확보.
  2. 데이터로더 배치 크기를 극단적으로 줄이거나 (예: batch_size=2), 더미 데이터로 오버피팅 테스트 진행.
  3. 에러 해결 후 정상 MAE 기록 확인.

## 4. AM/PM 2-run 계획 (단일 변수 통제 원칙)
- **AM Run**: `chgnet_head_finetune_freeze` 파인튜닝 스크립트의 **Batch Size**를 1/4로 줄여서 실행 (OOM 또는 차원 버그 확인용).
- **PM Run**: AM에 에러가 해결되었다면, **Learning Rate**를 기본값의 0.1배로 줄여서 `head_finetune_freeze` 안정성 초기 검증 진행. (에러 미해결시 `mp_gap` 데이터셋으로 교체하여 파이프라인 독립성 테스트)
