# MALTbot 매일 전략 업데이트 (2026-03-02)

## 1. 최근 성능 추이 요약 (최근 1~3일)
- **2026-02-28**: `chgnet_head_finetune_freeze` 모델이 MAE 0.467452라는 획기적인 성능 개선을 달성했습니다. 반면, Zero-shot 추론과 전체 파인튜닝(Full finetune)은 여전히 MAE 4.90~4.91 수준에 머물러 있습니다. 
- **2026-03-01**: ALIGNN 베이스라인(`alignn_fold0_agile`) 도입을 시도했으나 에러(METRIC=ERROR)가 발생하여 실행에 실패했습니다.

## 2. 타깃 선정
- **기본 타깃**: `mp_e_form` 유지를 제안합니다. 현재 CHGNet head finetuning에서 0.467의 우수한 결과가 나왔으므로 이를 안정화하고 극한으로 최적화하는 것이 우선입니다.
- **병행 제안**: `mp_e_form` 최적화가 정체기에 접어들 경우, 동일한 CHGNet 프리즈 전략을 `mp_gap`에 적용하여 다목적 전이학습 가능성을 확인하는 것을 추천합니다.

## 3. 다음 24h 최선 전략 (Top-1)
- **전략**: `chgnet_head_finetune_freeze`의 하이퍼파라미터(특히 Learning Rate) 미세 조정.
- **왜 (Why)**: 2월 28일 결과에서 백본을 고정하고 헤드만 학습했을 때 성능이 압도적으로 좋았습니다. 전체 구조를 학습하면 파라미터가 붕괴하거나 과적합되는 현상이 의심되므로, 성공이 입증된 헤드 파인튜닝의 최적점을 찾는 것이 가장 빠르고 확실한 Top-1 달성 방법입니다.
- **리스크**: 백본의 표현력 한계에 부딪혀 일정 수준(예: 0.3~0.4 MAE) 이하로 성능이 내려가지 않을 수 있습니다.
- **실행 단계**:
  1. 기존 0.467을 달성했던 설정(LR 등)을 베이스라인으로 확정.
  2. Learning rate를 조절하여 수렴 안정성 테스트.
  3. 실패한 ALIGNN 코드는 백그라운드에서 디버깅 진행.

## 4. 오늘 AM/PM 2-run 계획
- 변수 통제 원칙에 따라 **Learning Rate(학습률)** 단일 변수만 변경합니다.
- **AM Run**: `chgnet_head_finetune_freeze` (LR = 기존 대비 1/2 수준 감소). 세밀한 가중치 업데이트를 통해 0.467 이하로 MAE를 낮출 수 있는지 확인.
- **PM Run**: `chgnet_head_finetune_freeze` (LR = 기존 대비 2배 수준 증가). 학습 속도 향상 및 Local Minima 탈출 가능성을 확인하여 최적의 수렴 구간 파악.