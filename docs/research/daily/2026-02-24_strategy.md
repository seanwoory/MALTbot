# 2026-02-24 â€” Daily Strategy & Analysis (Updated: First Result MAE 4.754)

## 1) Analysis of the First Result: Why is MAE so high (4.754)?
- **Result:** `chgnet_pretrained_infer` (Zero-shot) finished with MAE = 4.754132 eV/atom.
- **Root Cause (Target Mismatch):** CHGNet natively predicts **Total Energy (per atom)**. However, the Matbench `mp_e_form` task requires predicting **Formation Energy (per atom)**. The difference is the sum of the standard reference state energies (AtomRefs) of the constituent elements. Without subtracting these elemental reference energies, the raw total energy prediction has a massive, systematic offset (several eV/atom).
- **Runtime Confirmation:** The run took ~44 minutes on an L4 GPU for zero-shot inference alone. This validates our prior analysis that the CPU-based `CrystalGraphConverter` is the primary bottleneck and must be cached globally to iterate effectively.

## 2) New Strict Strategy: Target Alignment Rule
To prevent this error in future modeling/engineering efforts:
- **Rule (Target Alignment Protocol):** Never evaluate or train a pretrained Universal Potential directly on a dataset without explicitly verifying and matching the physical definitions of the target quantities.
- **Actionable Steps for `mp_e_form`:** 
  1. If evaluating zero-shot, we must manually inject and subtract pre-calculated Elemental Reference Energies (AtomRefs) derived from the Materials Project.
  2. If finetuning, we must explicitly train the `composition_model` (AtomRef layer) and a linear readout head (e.g., using `chgnet_head_finetune_freeze`) to automatically absorb the reference energy offset and align the latent features to Formation Energy targets.

## 3) Does this change our 'Quantum Leap' strategy?
- **No, it validates and reinforces it.** Our previous strategy document (Leap #2: Precision Target Alignment) explicitly identified that Matbench targets Formation Energy while CHGNet predicts Total Energy. We anticipated needing a learned AtomRef correction.
- The massive error proves that "pure zero-shot" on `e_form` without AtomRef correction is structurally flawed.
- **Immediate Pivot:** We must prioritize the `chgnet_head_finetune_freeze` run. Training the AtomRef and readout head is strictly required to bridge the gap between Total E and Formation E.

## 4) Updated 24h Plan
- **Run #1 (Done):** `chgnet_pretrained_infer` -> MAE 4.754 (Confirmed target mismatch & CPU graph conversion bottleneck).
- **Run #2 (Next):** `chgnet_head_finetune_freeze`. Freeze the GNN backbone, but **unfreeze and train the AtomRef and readout head** to align the model to Formation Energy.
